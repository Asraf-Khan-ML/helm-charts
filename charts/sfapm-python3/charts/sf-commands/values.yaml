enableDebug: false
archiverEnabled: true
nameOverride: ""
fullnameOverride: ""
imagePullSecrets: []

podAnnotations:
  prometheus.io/scrape: "false"

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""
  # The names of the image pull secrets to be attached to this service account
  imagePullSecrets: []
  # disable mounting sa token inside pods
  automountServiceAccountToken: false
  
cloud:
  aws:
    enable: true
  gcs:
    enable: false
    # Set all of the below entities when Google-Service-Account is being used. Recommended approach unless K8s is running outside GCP (K8s outside GCP flow which will use JSON service-account-keys is not yet implemented)
    use_google_service_account: true
    service_account: "XXXX-compute@developer.gserviceaccount.com OR XXXX@XXXX.iam.gserviceaccount.com"
    region: us-west1
    zone: us-west1-c
  datacenter:
    enable: false

# set this to true/True if using pg bouncer
disable_server_side_cursors: false

sidecarLogger:
  enabled: true

  image:
    pullPolicy: Always

  resources:
    requests:
      cpu: 10m
      memory: 10Mi
    limits:
      cpu: 50m
      memory: 50Mi

commands:
  replicaCount: 2

  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 3
    # Below values are based on requests.
    # Hence value > 100 needs to be calculated based on set limits
    # Below values are calculated such that (limit / request) * 80 -> for 80% target utilization
    targetCPUUtilizationPercentage: 8000
    targetMemoryUtilizationPercentage: 240

  image:
    repository: snappyflowml/sfapm-commands
    tag: 'v1-1-6'
    pullPolicy: IfNotPresent

  resources:
    limits:
      cpu: 1000m
      memory: 768Mi
    requests:
      cpu: 100m
      memory: 256Mi

  service:
    type: ClusterIP
    port: 8000

commands_celery:
  env:
    C_FORCE_ROOT: true
    CELERY_OPTS: "-O fair --max-tasks-per-child 20 -P gevent"

  beat:
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 100Mi
    command: "celery -A command_server beat --scheduler django -l $LOG_LEVEL"

  default:
    resources:
      limits:
        cpu: 1000m
        memory: 768Mi
      requests:
        cpu: 100m
        memory: 100Mi
    replicaCount: 1
    command: "celery -A command_server worker -l $LOG_LEVEL -Q default -c10 $CELERY_OPTS"

postgresql:
  # enabled true deploys postgresql as statefull set on the same cluster
  enabled: true

  # if enabled false, chart will connect to external database
  # create two data bases with names snappyflow and vizbuilder
  # create a user and grant access to database snappyflow and vizbuilder
  # provide external section if enabled is false

  external:
    dbHost: ""
    dbPort: ""
    dbUser: ""
    dbPassword: ""

  image:
    pullPolicy: IfNotPresent

  persistence:
    enabled: true
    storageClass: ""
    size: 8Gi

  # when local postgresql is enabled below values are used to configure database
  rootPassword: postgres
  rootUser: postgres
  multidb: snappyflow;vizbuilder;commands;elasticsearch_manager
  multidbUser: snappyflow
  multidbUserPassord: maplelabs

redis:
  image:
    pullPolicy: IfNotPresent

sfapm:
  service:
    type: ClusterIP
    port: 8000

  replicaCount: 2

  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 3
    # Below values are based on requests.
    # Hence value > 100 needs to be calculated based on set limits
    # Below values are calculated such that (limit / request) * 80 -> for 80% target utilization
    targetCPUUtilizationPercentage: 1600
    targetMemoryUtilizationPercentage: 120
  
vizbuilder:
  service:
    type: ClusterIP
    port: 8000

sfapm_celery:
  env:
    C_FORCE_ROOT: true
    CELERY_OPTS: "-O fair --max-tasks-per-child 20 -P gevent"

  beat:
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 10m
        memory: 100Mi
    command: "celery -A snappyflow beat -l $LOG_LEVEL"

  default:
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 10m
        memory: 100Mi
    replicaCount: 1
    command: "celery -A snappyflow worker -l $LOG_LEVEL -Q default -c10 $CELERY_OPTS"

sfapmaffinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: "role"
                operator: In
                values:
                  - server
          topologyKey: "kubernetes.io/hostname"

podSecurityContext:
  {}
  
securityContext:
  {}
nodeSelector: {}

tolerations: []

affinity: {}

global:
  sfappname: sf-portal-app
  sfprojectname: snappyflow-app
  sfappname_key: snappyflow/appname
  sfprojectname_key: snappyflow/projectname
  nginx_geo_info_collection: true
  nginx_ua_parsing: true
  enable_sftrace: false
  key: ""

  sfNodeManager:
    enabled: false
    priorityClassName: sf-critical-pod
  sfScheduler:
    enabled: false
    schedulerName: sf-scheduler